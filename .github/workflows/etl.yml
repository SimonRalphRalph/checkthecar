name: Weekly ETL (publish cohort JSON)

on:
  schedule:
    - cron: "0 3 * * 1"   # Mondays 03:00 UTC
  workflow_dispatch: {}
  push:
    paths:
      - "etl/**"
      - "data_intermediate/**"
      - "data/**"
      - ".github/workflows/etl.yml"
      - "requirements.txt"

jobs:
  build-data:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    permissions:
      contents: write
    concurrency:
      group: etl-${{ github.ref }}
      cancel-in-progress: false

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Print ETL paths
        run: python -m etl.paths

      - name: Sanity check inputs
        run: |
          test -f data_intermediate/ved_bands.json || (echo "Missing data_intermediate/ved_bands.json" && exit 1)

      # If you want CI to download the three ZIPs each run, uncomment and provide DIRECT .zip URLs:
      # - name: Download MOT sources (results, failures, lookups)
      #   run: python -m etl.download_sources "https://..." "https://..." "https://..."

      - name: Ensure raw data folders (manual unzip supported)
        run: |
          test -d data_raw/results || (echo "Put Results CSVs under data_raw/results (or enable download step)"; exit 1)
          test -d data_raw/failures || (echo "Put Failures CSVs under data_raw/failures"; exit 1)
          test -d data_raw/lookups || (echo "Put Lookups CSVs under data_raw/lookups"; exit 1)

      - name: Ingest Results → Parquet (decoded)
        run: python -m etl.ingest_results

      - name: Ingest Failure Items → Parquet (bucketed)
        run: python -m etl.ingest_failures

      - name: Seed alias file (safe no-op if already complete)
        run: python -m etl.alias_seed || true

      - name: Aggregate MOT → Parquet
        run: python -m etl.aggregate_mot

      - name: Join & publish JSON to public/data
        run: python -m etl.join_publish

      - name: Quick validation of outputs
        run: |
          test -d public/data || (echo "No public/data directory" && exit 1)
          COUNT=$(find public/data -type f -name "*.json" | wc -l)
          echo "JSON files: $COUNT"
          if [ "$COUNT" -eq "0" ]; then
            echo "No cohort JSONs were produced"; exit 2
          fi

      - name: Commit cohort JSON
        run: |
          if [ -n "$(git status --porcelain public/data)" ]; then
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"
            git add public/data
            git commit -m "ETL: refresh cohort JSON [skip ci]"
            git push
          else
            echo "No changes to commit."
          fi