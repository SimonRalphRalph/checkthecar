name: Weekly ETL (publish cohort JSON)

on:
  schedule:
    - cron: "0 3 * * 1"   # Mondays 03:00 UTC
  workflow_dispatch:
  push:
    paths:
      - "etl/**"
      - "data_intermediate/**"
      - "data/**"
      - ".github/workflows/etl.yml"
      - "requirements.txt"

jobs:
  build-data:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    permissions:
      contents: write
    concurrency:
      group: etl-${{ github.ref }}
      cancel-in-progress: false

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Print ETL paths
        run: python -m etl.paths

      - name: Sanity check inputs
        run: |
          test -f data_intermediate/ved_bands.json || (echo "Missing data_intermediate/ved_bands.json" && exit 1)

      # ---- Download using curl + unzip (robust & fast) ----
      - name: Download MOT sources (results, failures, lookups)
        run: |
          set -euo pipefail
          sudo apt-get update -y
          sudo apt-get install -y unzip xxd

          mkdir -p data_raw/results data_raw/failures data_raw/lookups

          try_download() {
            url="$1"; out="$2"
            echo "Fetching: $url"
            http_code=$(curl -w '%{http_code}' -fsSL --retry 5 --retry-delay 2 --retry-connrefused -o "$out" "$url" || true)
            echo "HTTP $http_code"
            if [ "$http_code" != "200" ]; then
              echo "---- first 200 bytes ----"
              head -c 200 "$out" 2>/dev/null || true
              echo ""
              return 1
            fi
            if ! head -c 2 "$out" | grep -q 'PK'; then
              echo "Not a ZIP (no PK header). First 64 bytes (hex):"
              head -c 64 "$out" | xxd
              return 1
            fi
            return 0
          }

          # Candidates for RESULTS (2024)
          RES_OUT=/tmp/results.zip
          RES_URLS=(
            "https://edh-dvsa-data-gov-uk-files-prod.s3.eu-west-1.amazonaws.com/MOT+testing+data+results+%282024%29.zip"
            "https://edh-dvsa-data-gov-uk-files-prod.s3.eu-west-1.amazonaws.com/MOT%2Btesting%2Bdata%2Bresults%2B%282024%29.zip"
          )

          ok=0
          for u in "${RES_URLS[@]}"; do
            if try_download "$u" "$RES_OUT"; then ok=1; break; fi
          done
          if [ "$ok" -ne 1 ]; then
            echo "ERROR: Could not download RESULTS (2024) from any candidate URL."
            exit 9
          fi
          unzip -o -q "$RES_OUT" -d data_raw/results

          # Candidates for FAILURES (2024) – note the capital T in 'Testing'
          FAIL_OUT=/tmp/failures.zip
          FAIL_URLS=(
            "https://edh-dvsa-data-gov-uk-files-prod.s3.eu-west-1.amazonaws.com/MOT+Testing+data+failure+item+%282024%29.zip"
            "https://edh-dvsa-data-gov-uk-files-prod.s3.eu-west-1.amazonaws.com/MOT%2BTesting%2Bdata%2Bfailure%2Bitem%2B%282024%29.zip"
          )

          ok=0
          for u in "${FAIL_URLS[@]}"; do
            if try_download "$u" "$FAIL_OUT"; then ok=1; break; fi
          done
          if [ "$ok" -ne 1 ]; then
            echo "ERROR: Could not download FAILURE ITEMS (2024) from any candidate URL."
            exit 9
          fi
          unzip -o -q "$FAIL_OUT" -d data_raw/failures

          # Lookup tables
          LOOK_OUT=/tmp/lookups.zip
          LOOK_URLS=(
            "https://edh-dvsa-data-gov-uk-files-prod.s3.eu-west-1.amazonaws.com/lookup.zip"
          )

          ok=0
          for u in "${LOOK_URLS[@]}"; do
            if try_download "$u" "$LOOK_OUT"; then ok=1; break; fi
          done
          if [ "$ok" -ne 1 ]; then
            echo "ERROR: Could not download LOOKUP TABLES from any candidate URL."
            exit 9
          fi
          unzip -o -q "$LOOK_OUT" -d data_raw/lookups

      - name: Ingest Results → Parquet (decoded)
        run: python -m etl.ingest_results

      - name: Ingest Failure Items → Parquet (bucketed)
        run: python -m etl.ingest_failures

      - name: Seed alias file (safe no-op if already complete)
        run: python -m etl.alias_seed || true

      - name: Commit alias updates (if any)
        run: |
          if [ -n "$(git status --porcelain data/model_aliases.csv)" ]; then
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"
            git add data/model_aliases.csv
            git commit -m "Alias seed: update model_aliases.csv [skip ci]" || true
            git push || true
          else
            echo "No alias changes."
          fi

      - name: Aggregate MOT → Parquet
        run: python -m etl.aggregate_mot

      # Package intermediates so the sharded publish job can use them
      - name: Upload ETL intermediates
        uses: actions/upload-artifact@v4
        with:
          name: etl-intermediates
          if-no-files-found: error
          retention-days: 3
          path: |
            data_intermediate/mot_agg.parquet
            data_intermediate/failure_shares.parquet
            data_intermediate/recalls.parquet
            data_intermediate/vca.parquet
            data_intermediate/ved_bands.json

  publish:
    needs: build-data
    runs-on: ubuntu-latest
    permissions:
      contents: write
    strategy:
      fail-fast: false
      matrix:
        shard: [0,1,2,3,4,5,6,7]   # 8 shards in parallel
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install deps
        run: |
         python -m pip install --upgrade pip
         pip install -r requirements.txt

      - name: Download ETL intermediates
        uses: actions/download-artifact@v4
        with:
          name: etl-intermediates
          path: data_intermediate

      - name: Join & publish JSON (sharded)
        env:
          ETL_SHARD: "${{ matrix.shard }}"
          ETL_SHARDS: "8"
          PYTHONFAULTHANDLER: "1"
          # ETL_MAX_COHORTS: "400"   # (leave commented unless you want to cap)
        run: python -u -m etl.join_publish

      - name: Commit cohort JSON for this shard
        run: |
          if [ -n "$(git status --porcelain public/data)" ]; then
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"
            git add public/data
            git commit -m "ETL: publish JSON shard ${{ matrix.shard }} [skip ci]" || true
            # handle rare push races between shards
            git pull --rebase || true
            git push || (git pull --rebase && git push) || true
          else
            echo "No changes to commit."
          fi
