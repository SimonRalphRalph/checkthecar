name: Weekly ETL (publish cohort JSON)

on:
  schedule:
    - cron: "0 3 * * 1"   # Mondays 03:00 UTC
  workflow_dispatch: {}
  push:
    paths:
      - "etl/**"
      - "data_intermediate/**"
      - "data/**"
      - ".github/workflows/etl.yml"
      - "requirements.txt"

jobs:
  build-data:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    permissions:
      contents: write
    concurrency:
      group: etl-${{ github.ref }}
      cancel-in-progress: false

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Print ETL paths
        run: python -m etl.paths

      - name: Sanity check inputs
        run: |
          test -f data_intermediate/ved_bands.json || (echo "Missing data_intermediate/ved_bands.json" && exit 1)

      - name: Download MOT sources (results, failures, lookups)
        run: |
          python -m etl.download_sources \
            "https://edh-dvsa-data-gov-uk-files-prod.s3.eu-west-1.amazonaws.com/MOT%2Btesting%2Bdata%2Bresults%2B%282024%29.zip" \
            "https://edh-dvsa-data-gov-uk-files-prod.s3.eu-west-1.amazonaws.com/MOT%2BTesting%2Bdata%2Bfailure%2Bitem%2B%282024%29.zip" \
            "https://edh-dvsa-data-gov-uk-files-prod.s3.eu-west-1.amazonaws.com/lookup.zip"

      - name: Ensure raw data folders (post-download)
        run: |
          test -d data_raw/results   || (echo "Expected data_raw/results after download"; exit 1)
          test -d data_raw/failures  || (echo "Expected data_raw/failures after download"; exit 1)
          test -d data_raw/lookups   || (echo "Expected data_raw/lookups after download"; exit 1)

      - name: Ingest Results → Parquet (decoded)
        run: python -m etl.ingest_results

      - name: Ingest Failure Items → Parquet (bucketed)
        run: python -m etl.ingest_failures

      - name: Seed alias file (safe no-op if already complete)
        run: python -m etl.alias_seed || true

      - name: Commit alias updates (if any)
        run: |
          if [ -n "$(git status --porcelain data/model_aliases.csv)" ]; then
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"
            git add data/model_aliases.csv
            git commit -m "Alias seed: update model_aliases.csv [skip ci]" || true
            git push || true
          else
            echo "No alias changes."
          fi

      - name: Aggregate MOT → Parquet
        run: python -m etl.aggregate_mot

      - name: Join & publish JSON to public/data
        run: python -m etl.join_publish

      - name: Quick validation of outputs
        run: |
          test -d public/data || (echo "No public/data directory" && exit 1)
          COUNT=$(find public/data -type f -name "*.json" | wc -l)
          echo "JSON files: $COUNT"
          if [ "$COUNT" -eq "0" ]; then
            echo "No cohort JSONs were produced"; exit 2
          fi

      - name: Commit cohort JSON
        run: |
          if [ -n "$(git status --porcelain public/data)" ]; then
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"
            git add public/data
            git commit -m "ETL: refresh cohort JSON [skip ci]"
            git push
          else
            echo "No changes to commit."
          fi
