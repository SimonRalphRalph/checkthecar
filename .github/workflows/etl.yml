name: Weekly ETL
on:
  schedule:
    - cron: '0 3 * * 1'  # Mondays 03:00 UTC (~04:00/03:00 UK depending on DST)
  workflow_dispatch:

permissions:
  contents: write

jobs:
  etl:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with: { fetch-depth: 0 }
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - name: Install Python deps
        run: |
          python -m pip install -U pip
          pip install pandas pyarrow requests python-dateutil rapidfuzz tqdm
      - name: Build aggregates (sample flow)
        run: |
          python - <<'PY'
from etl.aggregate_mot import compute_aggregates
from etl.recalls import load_recalls, aggregate_recalls
from etl.vca_co2 import load_vca_csv
from etl.export_json import write_cohort_json
import pandas as pd
from pathlib import Path

# In CI, replace fixtures with real parquet reads and joins.
df = pd.DataFrame([
  {"make":"Ford","model":"Fiesta","firstUseDate":"2013-06-01","testDate":"2023-05-10","odometerReading":72000,"odometerReadingUnits":"miles","testResult":"PASS","rfrAndComments":"","fuelType":"Petrol"},
  {"make":"Ford","model":"Fiesta","firstUseDate":"2013-06-01","testDate":"2024-05-11","odometerReading":79000,"odometerReadingUnits":"miles","testResult":"FAIL","rfrAndComments":"BRS123","fuelType":"Petrol"},
])
agg = compute_aggregates(df)
rec = aggregate_recalls(load_recalls())
vca = load_vca_csv('https://example.com/vca.csv')  # replace with real URL
write_cohort_json(Path('public/data'), agg, rec, vca)
PY
      - name: Commit data updates
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          if [ -n "$(git status --porcelain public/data)" ]; then
            git add public/data
            git commit -m "ETL: refresh cohort JSONs"
            git push
          else
            echo "No changes"
          fi
